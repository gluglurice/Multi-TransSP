import torch
from torch import nn, einsum

from einops import rearrange, reduce
from rotary_embedding_torch import apply_rotary_emb, RotaryEmbedding


# helper functions
def exists(val):
    return val is not None


def default(val, d):
    return val if exists(val) else d


# helper classes
class PreNorm(nn.Module):
    def __init__(self, dim, fn):
        super().__init__()
        self.norm = nn.LayerNorm(dim)
        self.fn = fn

    def forward(self, x, **kwargs):
        x = self.norm(x)
        return self.fn(x, **kwargs)


# blocks
def FeedForward(dim, mult=4):
    return nn.Sequential(
        nn.Linear(dim, dim * mult),
        nn.GELU(),
        nn.Linear(dim * mult, dim)
    )


class FastAttention(nn.Module):
    def __init__(
            self,
            dim,
            *,
            heads=8,
            dim_head=64,
            max_seq_len=None,
            pos_emb=None
    ):
        super().__init__()
        inner_dim = heads * dim_head
        self.heads = heads
        self.scale = dim_head ** -0.5

        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)

        # rotary positional embedding

        assert not (exists(pos_emb) and not exists(
            max_seq_len)), 'max_seq_len must be passed in if to use rotary positional embeddings'

        self.pos_emb = pos_emb
        self.max_seq_len = max_seq_len

        # if using relative positional encoding,
        # make sure to reduce pairs of consecutive feature num_patches before doing projection to attention logits

        kv_attn_proj_divisor = 1 if not exists(pos_emb) else 2

        self.to_q_attn_logits = nn.Linear(dim_head, 1, bias=False)  # for projecting queries to query attention logits
        self.to_k_attn_logits = nn.Linear(dim_head // kv_attn_proj_divisor, 1,
                                          bias=False)  # for projecting keys to key attention logits

        # final transformation of values to "r" as in the paper

        self.to_r = nn.Linear(dim_head // kv_attn_proj_divisor, dim_head)

        self.to_out = nn.Linear(inner_dim, dim)

    def forward(self, x, mask=None):
        n, device, h, use_rotary_emb = x.shape[1], x.device, self.heads, exists(self.pos_emb)

        qkv = self.to_qkv(x).chunk(3, dim=-1)
        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=h), qkv)

        mask_value = -torch.finfo(x.dtype).max
        mask = rearrange(mask, 'b n -> b () n')

        # if relative positional encoding is needed
        if use_rotary_emb:
            freqs = self.pos_emb(torch.arange(self.max_seq_len, device=device), cache_key=self.max_seq_len)
            freqs = rearrange(freqs[:n], 'n d -> () () n d')
            q_aggr, k_aggr, v_aggr = map(lambda t: apply_rotary_emb(freqs, t), (q, k, v))
        else:
            q_aggr, k_aggr, v_aggr = q, k, v

        # calculate query attention logits
        q_attn_logits = rearrange(self.to_q_attn_logits(q), 'b h n () -> b h n') * self.scale
        q_attn_logits = q_attn_logits.masked_fill(~mask, mask_value)
        q_attn = q_attn_logits.softmax(dim=-1)

        # calculate global query token
        global_q = einsum('b h n, b h n d -> b h d', q_attn, q_aggr)
        global_q = rearrange(global_q, 'b h d -> b h () d')

        # bias keys with global query token
        k = k * global_q

        # if using rotary embeddings, do an inner product between adjacent pairs in the feature num_patches
        if use_rotary_emb:
            k = reduce(k, 'b h n (d r) -> b h n d', 'sum', r=2)

        # now calculate key attention logits
        k_attn_logits = rearrange(self.to_k_attn_logits(k), 'b h n () -> b h n') * self.scale
        k_attn_logits = k_attn_logits.masked_fill(~mask, mask_value)
        k_attn = k_attn_logits.softmax(dim=-1)

        # calculate global key token
        global_k = einsum('b h n, b h n d -> b h d', k_attn, k_aggr)
        global_k = rearrange(global_k, 'b h d -> b h () d')

        # bias the values
        u = v_aggr * global_k

        # if using rotary embeddings, do an inner product between adjacent pairs in the feature num_patches
        if use_rotary_emb:
            u = reduce(u, 'b h n (d r) -> b h n d', 'sum', r=2)

        # transformation step
        r = self.to_r(u)

        # paper then says to add the queries as a residual
        r = r + q

        # combine heads
        r = rearrange(r, 'b h n d -> b n (h d)')
        return self.to_out(r)


# main class
class Fastformer(nn.Module):
    def __init__(
            self,
            *,
            num_tokens,
            dim,
            depth,
            max_seq_len,
            heads=8,
            dim_head=64,
            ff_mult=4,
            absolute_pos_emb=False
    ):
        super().__init__()
        self.token_emb = nn.Embedding(num_tokens, dim)

        # positional embeddings

        self.abs_pos_emb = nn.Embedding(max_seq_len, dim) if absolute_pos_emb else None

        layer_pos_emb = None
        if not absolute_pos_emb:
            assert (dim_head % 4) == 0, 'num_patches of the head must be divisible by 4 to use rotary embeddings'
            layer_pos_emb = RotaryEmbedding(dim_head // 2)

        # layers

        self.layers = nn.ModuleList([])

        for _ in range(depth):
            attn = FastAttention(dim, dim_head=dim_head, heads=heads, pos_emb=layer_pos_emb, max_seq_len=max_seq_len)
            ff = FeedForward(dim, mult=ff_mult)

            self.layers.append(nn.ModuleList([
                PreNorm(dim, attn),
                PreNorm(dim, ff)
            ]))

        # weight tie projections across all layers

        first_block, _ = self.layers[0]
        for block, _ in self.layers[1:]:
            block.fn.to_q_attn_logits = first_block.fn.to_q_attn_logits
            block.fn.to_k_attn_logits = first_block.fn.to_k_attn_logits

        # to logits

        self.to_logits = nn.Sequential(
            nn.LayerNorm(dim),
            nn.Linear(dim, 1)
        )

    def forward(self, x, mask=None):
        n, device = x.shape[1], x.device

        if exists(self.abs_pos_emb):
            pos_emb = self.abs_pos_emb(torch.arange(n, device=device))
            x = x + rearrange(pos_emb, 'n d -> () n d')

        for attn, ff in self.layers:
            x = attn(x, mask=mask) + x
            x = ff(x) + x

        return self.to_logits(x)


if __name__ == "__main__":
    model = Fastformer(num_tokens=256, dim=121, depth=2, max_seq_len=256, absolute_pos_emb=False)

    a = torch.ones(1, 256, 121, dtype=torch.float32)
    mask = torch.ones([1, 256]).bool()

    a = model(a, mask=mask)
    print(a)
